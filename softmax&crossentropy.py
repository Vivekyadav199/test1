"""Softmax&CrossEntropy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r9g0z0L6R4wKWuTwJ9u-RcoAVxnKYQ-R
"""

import torch
import torch.nn as n# Example code using "nn" variable
nn = 10  # Assuming nn is an integer variable

# Accessing the "nn" variable
print(nn)  # You can print it or use it in any other wayn
import numpy as np
import pandas as pd
def softmax(x):
  if isinstance(x, (list, np.ndarray)):
        exp_values = np.exp(x - np.max(x, axis=0))
        return exp_values / np.sum(exp_values, axis=0)
  elif isinstance(x, (int, float)):
        exp_values = np.exp([x])
        return exp_values / np.sum(exp_values)
  else:

    return

x=np.array([2.0,1.0,0.1])
outputs=softmax(x)
print('softmax numpy:',outputs)


x=torch.tensor([2.0,1.0,0.1])
outputs = torch.softmax(x, dim=0)
print(outputs)


class Calculator:

    def add(self, x, y):
        return x + y

    def subtract(self, x, y):
        return x - y

    def multiply(self, x, y):
        return x * y

    def divide(self, x, y):
        if y != 0:
            return x / y
        return "Cannot divide by zero"


calc = Calculator()

result1 = calc.add(5, 7)
result2 = calc.subtract(10, 3)
result3 = calc.multiply(5, 6)
result4 = calc.divide(10, 2)

print(result1)
print(result2)
print(result3)
print(result4)





































































